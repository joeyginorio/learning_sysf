% For review submission
%\documentclass[acmsmall,review,anonymous]{acmart}
\documentclass[acmsmall]{acmart}

\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[acmsmall]{acmart}\settopmatter{}


% Load any additional packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath,amsthm,amsfonts,wasysym,bussproofs}
\usepackage{csquotes}
\usepackage{booktabs,changepage}
\usepackage{lipsum}
\usepackage{setspace}

\usepackage{titlesec,epigraph, listings}
\usepackage{etoolbox}
\usepackage{enumitem}

% Commands %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\lam}{\lambda}

\renewenvironment{proof}
    {\textit{Proof.}}
    {\qed\\}
    
\newtheoremstyle{mytheoremstyle} % name
    {}                    % Space above
    {}                    % Space below
    {\itshape}                   % Body font
    {}                 % Indent amount
    {\bfseries}                  % Theorem head font
    {.}                          % Punctuation after theorem head
    {\newline}                      % Space after theorem head
    {}                           % Theorem head spec (can be left empty, meaning ‘normal’)
\theoremstyle{mytheoremstyle}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}




%% Journal information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{CONF} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2018}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format must update the '\documentclass' and
%% topmatter commands above; see 'acmart-sigplanproc-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption


\begin{document}

%% Title information
\title{Learning in System F (Synthesis Pearl)}

%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% %% Author with single affiliation.
%% \author{Joey Velez-Ginorio}
%% \affiliation{
%%   \department{Brain \& Cognitive Sciences}
%%   \institution{Massachusetts Institute of Technology}
%%   \streetaddress{43 Vassar St.}
%%   \city{Cambridge}
%%   \state{Massachusetts}
%%   \postcode{02139}
%%   \country{U.S.A.}                    %% \country is recommended
%% }
%% \email{joeyv@mit.edu}   

%% %% Author with single affiliation.
%% \author{Nada Amin}
%% \affiliation{
%%   \department{John A. Paulson School of Engineering and Applied Sciences}
%%   \institution{Harvard University}
%%   \streetaddress{29 Oxford St.}
%%   \city{Cambridge}
%%   \state{Massachusetts}
%%   \postcode{02138}
%%   \country{U.S.A.}                    %% \country is recommended
%% }
%% \email{namin@seas.harvard.edu}
\author{}

%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
Program synthesis, type inhabitance, inductive programming, and theorem proving. Different names for the same problem: learning programs from data. Sometimes the programs are proofs, sometimes they’re terms. Sometimes data are examples, and sometimes they’re types. Yet the aim is the same. We want to construct a program which satisfies some data. We want to learn a program.

What might a programming language look like, if its programs could also be learned? We give it data, and it learns a program from it. This work shows that System F yields a simple approach for learning from types and examples. Beyond simplicity, System F gives us a guarantee on the soundness and completeness of learning. We learn correct programs, and can learn all observationally distinct programs in System F. Unlike previous works, we don't restrict what examples can be. As a result, we show how to learn eventually arbitrary higher-order programs in System F from types and examples. 

\end{abstract}


%% Keywords
%% comma separated list
\keywords{Program Synthesis, Type Theory, Inductive Programming} 
\maketitle


\section{Introduction}
\subsection{A tricky learning problem}

Imagine we're teaching you a program. Your only data is the type $nat \!\to\! nat$. It takes a natural number, and returns a natural number. Any ideas? Perhaps a program which computes...
$$f(x) = x, \;\;\;\;\;\;f(x) = x + 1,\;\;\;\;\;\; f(x) = x + 2,\;\;\;\;\;\; f(x) = x + \cdots$$
The good news is that $f(x) = x + 1$ is correct. The bad news is that the data let you learn a slew of other programs too. It doesn't constrain learning enough if we want to teach $f(x) = x + 1$. As teachers, we can provide better data.

Round 2. Imagine we're teaching you a program. But this time we give you an example of the program's behavior. Your data are the type $nat \!\to\! nat$ and an example $f(1) = 2$. It takes a natural number, and seems to return its successor. Any ideas? Perhaps a program which computes...
$$f(x) = x + 1,\;\;\;\;\;\; f(x) = x + 2 - 1,\;\;\;\;\;\; f(x) = x + 3 - 2,\;\;\;\;\;\;\cdots$$
The good news is that $f(x) = x + 1$ is correct. And so are all the other programs, as long as we're agnostic to some details. Types and examples impose useful constraints on learning. It's the data we use when learning in System F \cite{girard1989proofs}.

Existing work can learn successor from similar data \cite{osera2015program, polikarpova2016program}. But suppose $nat$ is a church encoding. For some base type $A$, $nat \coloneqq (A \to A) \to (A \to A)$. Natural numbers are then higher-order functions. They take and return functions. In this context, existing work can no longer learn successor. 

\subsection{A way forward}

The difficulty is with how to handle functions in the return type. The type $nat \!\to\! nat$ returns a function, a program of type $nat$. To learn correct programs, you need to ensure candidates are the correct type or that they obey examples. Imagine we want to verify that our candidate program $f$ obeys $f(1)=2$. With the church encoding, $f(1)$ is a function, and so is $2$. To check $f(1)=2$ requires that we decide function equality---which is undecidable in a Turing-complete language \cite{sipser2006introduction}. Functions in the return type create this issue. There are two ways out.

\begin{enumerate}
\item Don't allow functions in the return type, keep Turing-completeness.
\item Allow functions in the return type, leave Turing-completeness.
\end{enumerate}

Route 1 is the approach of existing work. They don't allow functions in the return type, but keep an expressive Turing-complete language for learning. This can be a productive move, as many interesting programs don't return functions.

Route 2 is the approach we take. We don't impose restrictions on the types or examples we learn from. We instead sacrifice Turing-completeness. We choose a language where function equality is decidable, but still expressive enough to learn interesting programs. Our work shows that this too is a productive move, as many interesting programs return functions. This route leads us to several contributions:
\begin{itemize}
\item Detail how to learn arbitrary higher-order programs in System F.
\item Prove the soundness and completeness of learning.
\item Provide an implementation of learning, extending strong theoretical guarantees in practice.
\end{itemize}


\section{System F}
We assume you are familiar with System F, the polymorphic lambda calculus. You should know its syntax, typing, and evaluation. If you don't, we co-opt its specification in \cite{pierce2002types}. For a comprehensive introduction we defer the confused or rusty there. Additionally, we provide the specification and relevant theorems in the appendix.

Our focus in this section is to motivate System F: its syntax, typing, and evaluation. And why properties of each are advantageous for learning. Treat this section as an answer to the following question: 
\begin{displayquote}
\textit{Why learn in System F?}
\centering
\end{displayquote}
\subsection{Syntax}

% minimal syntax
% expressive enough to encode the useful things
System F's syntax is simple. There aren't many syntactic forms. Whenever we state, prove, or implement things in System F we often use structural recursion on the syntax. A minimal syntax means we are succint when we state, prove, or implement those things.

While simple, the syntax is still expressive. We can encode many staples of typed functional programming: algebraic data types, inductive types, and more \cite{pierce2002types}. For example, consider this encoding of products:
\begin{align*}
\tau_1 \times \tau_2 \,&::=\; \forall \alpha.(\tau_1 \to \tau_2 \to \alpha) \to \alpha\\
\langle e_1,e_2 \rangle &::=\; \Lambda \alpha.\lambda f\!:\!(\tau_1 \to \tau_2 \to \alpha).fe_1e_2 
\end{align*}

\subsection{Typing}
% Typed programs are safe : progress + preservation. Later, we leverage this to ensure the safety of programs we learn.
System F is safe. Its typing ensures both progress and preservation, i.e. that well-typed programs do not get stuck and that they do not change type \cite{pierce2002types}. When we introduce learning, we lift this safety and extend it to programs we learn. Because we use this safety in later proofs, we state the progress and preservation theorems in the appendix.

\subsection{Evaluation}
% strongly normalizing, show example of functions equivalent
System F is strongly normalizing. All its programs terminate. As a result, we can use a simple procedure for deciding equality of programs (including functions). 
\begin{enumerate}
\item Run both programs until they terminate.
\item Check if they share the same normal form, up to alpha-equivalence (renaming of variables).
\item If they do, they are equal. Otherwise, unequal.
\end{enumerate}
For example, this decision procedure renders these programs equal:
$$\lambda x\!:\!\tau.x \;\;=_\beta\;\; (\lambda y\!:\!(\tau\to\tau).y)\lambda z\!:\!\tau.z$$
The decision procedure checks that two programs exist in the transitive reflexive closure of the evaluation relation. This only works because programs always terminate, a property we formally state in the appendix.
% Strong normalization means we can decide function equality through the fdollowwing procedure: evaluate term, compare up to alpha equivalence.

\section{Learning from Types}
% present the augmented relation
% state completeness/soundness result? or prove...

% dont need a figure, but state the normal form syntax like osera does
% then introduce the learning from types.




\begin{figure}[ht]
\centering
\setlength{\tabcolsep}{12pt}
\begin{tabular}{l r  l r}
\specialrule{.1em}{0em}{.2em}
\specialrule{.1em}{0em}{1em}
    \Large \textsc{Learning} & 
    &  & \framebox{$\Gamma \vdash \tau \rightsquigarrow e$}\\
    & & \\
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$x:\tau \in \Gamma$}
            \RightLabel{\textsc{(L-Var)}}
        \UnaryInfC{$\Gamma \vdash \tau \rightsquigarrow x$}
        \DisplayProof
    } &
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$\Gamma,\alpha \vdash \tau \rightsquigarrow e$}
            \RightLabel{\textsc{(G-TAbs)}}
        \UnaryInfC{$\Gamma \vdash \forall\alpha.\tau \rightsquigarrow \lam \alpha.e$}
        \DisplayProof
    }
    \\
    & &\\
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$\Gamma,x{:}\tau_1 \vdash \tau_2 \rightsquigarrow e_2$}
            \RightLabel{\textsc{(L-Abs)}}
        \UnaryInfC{$\Gamma \vdash \tau_1 \to \tau_2 \rightsquigarrow \lam x{:}\tau_1.e_2$}
        \DisplayProof
    } &
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$\Gamma \vdash \forall\alpha.\tau_1 \rightsquigarrow e$}
            \RightLabel{\textsc{(L-TApp)}}
        \UnaryInfC{$\Gamma \vdash [\tau_2/\alpha]\tau_1 \rightsquigarrow e\lceil\tau_2\rceil$}
        \DisplayProof
    }
    \\
    & &\\
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$\Gamma \vdash \tau_1 \to \tau_2 \rightsquigarrow e_1$}
        \AxiomC{$\Gamma \vdash \tau_1 \rightsquigarrow e_2$}
            \RightLabel{\textsc{(L-App)}}
        \BinaryInfC{$\Gamma \vdash \tau_2 \rightsquigarrow e_1e_2$}
        \DisplayProof
    } \\
    & \\
\specialrule{.1em}{1em}{0em}
\end{tabular}
\caption{Learning from types in System F}
    \label{fig:learning}
\end{figure}

\section{Learning from Examples, Declaratively}

\subsection{What are examples?}

Examples $[\chi_1,\dots,\chi_n]$ are lists of tuples, containing the inputs and output to a program. For instance $[\langle1,1\rangle]$ describes an example whose input is $1$ and output is $1$. If I want to specify more than one example, I can package examples $\chi$ into the list: $[\langle 1,1\rangle, \langle2,2\rangle]$. Here I have two examples, each with one input and one output. In general, examples take the form \vspace{-1.0em}
\begin{singlespace}
$$\chi ::= \langle e, \chi\rangle \mid \langle e,Nil\rangle$$
\end{singlespace}
where $e$ is an arbitrary program in System F and $\chi$ is an example. This syntax for examples lets us construct examples with arbitrary numbers of inputs, e.g. $\langle10,10,20\rangle \equiv \langle 10, \langle 10, \langle 20,Nil\rangle\rangle\rangle.$ Note that when examples have multiple inputs I use the short-hand notation for describing an $n-$tuple, $\langle10,10,20\rangle$ in lieu of full notation $ \langle 10, \langle 10, \langle 20,Nil\rangle\rangle\rangle$. Likewise when an example is merely $\langle e,Nil\rangle$, I use the short-hand $\langle e\rangle$---as $Nil$ can be interpreted as the empty element.

For an example $\chi \equiv \langle e_1,\dots,e_n\rangle$, an ordered list of inputs is given by $e_1,\dots,e_{n-1}$. The last index always denotes an output. An example satisfies or describes a program, if when the ordered list of inputs $e_1,\dots,e_{n-1}$ is applied to a program $e$, it is equivalent to the output $e_n$. That is,\vspace{-1.0em}
\begin{singlespace}
$$(((e\,e_1)e_2)\dots e_{n-1}) =_\beta e_n$$
\end{singlespace}
For instance, $\chi \equiv \langle 1,1\rangle$ satisfies the identity program $\lam x{:}nat.x$ because \vspace{-1.0em}
\begin{singlespace}
$$(\lam x{:}nat.x)1 =_\beta 1$$
\end{singlespace}
Similarly, a list of examples $[\chi_1,\dots,\chi_n]$ satisfies some program if each example in the list satisfies the program. Note that with this notion of satisfaction, we can construct examples which satisfy any program $e$, that is $\langle e\rangle$. It's an example with no input, and whose output is $e$. Because no inputs can be applied, and that $e =_\beta e$, $\langle e\rangle$ satisfies $e$.

With the learning relation, we can ask whether \textsc{identity} is learnable given a context, type, and examples. \textsc{identity} is a program which takes a natural number and returns it. \vspace{-1.0em}
\begin{singlespace}
$$\cdot \vdash nat \!\to\! nat \rhd \langle\langle1,1\rangle,\langle2,2\rangle\rangle \rightsquigarrow \blacksquare$$
\end{singlespace}
Examples are stored as tuples. They describe possible worlds, one where our program's input is 1 and the other where our program's input is 2. Throughout learning we need a way to keep track of these distinct worlds. So our first step is always to duplicate $\blacksquare$, so that there is one per example. \vspace{-1.0em}
\begin{singlespace}
$$\cdot \vdash list \,nat \!\to\! nat \rhd \langle\langle1,1\rangle,\langle2,2\rangle\rangle \rightsquigarrow [\blacksquare, \blacksquare]$$
\end{singlespace}
Let's refine these worlds, by applying them to their respective inputs. We extract the inputs from each example tuple.\vspace{-1.0em}
\begin{singlespace}
$$1{:}nat, 2{:}nat \vdash list \,nat \rhd \langle\langle1\rangle,\langle2\rangle\rangle \rightsquigarrow [(\blacksquare)1, (\blacksquare)2]$$
\end{singlespace}
Because $\blacksquare$ is applied to an argument, we know it must be an abstraction. Hence, we can also claim: \vspace{-1.0em}
\begin{singlespace}
$$1{:}nat, 2{:}nat \vdash list \,nat \rhd \langle\langle1\rangle,\langle2\rangle\rangle \rightsquigarrow [(\lam x{:}nat.\blacksquare)1, (\lam x{:}nat.\blacksquare)2]$$
\end{singlespace}
Now that we've ran out of inputs in our examples, the problem becomes how to generate a program which satisfy the outputs left in the example tuples:\vspace{-1.0em}
\begin{singlespace}
$$1{:}nat, 2{:}nat \vdash list \,nat\, \rightsquigarrow [(\lam x{:}nat.\blacksquare)1, (\lam x{:}nat.\blacksquare)2]$$
$$(\lam x{:}nat.\blacksquare)1 =_\beta 1\;\land\;(\lam x{:}nat.\blacksquare)2=_\beta2$$
\end{singlespace}
Given the constraints on well-typed terms, it's easy to find $x$ to fill the body of the abstraction. This will become clear in the formal proof to follow. \vspace{-1.0em}
\begin{singlespace}
$$1{:}nat, 2{:}nat \vdash list \,nat\, \rightsquigarrow [(\lam x{:}nat.x)1, (\lam x{:}nat.x)2]$$
$$(\lam x{:}nat.x)1 =_\beta 1\;\land\;(\lam x{:}nat.x)2=_\beta2$$
\end{singlespace}
Having satisfied the outputs from our examples, we've informally shown $\textsc{identity} \equiv \lam x{:}nat.x$ is learnable in System F. And all the machinery comes from types and operators we can encode in System F: list and product types along with their constructors and deconstructors.

\begin{figure}[h!]
\centering
\setlength{\tabcolsep}{12pt}
\begin{tabular}{l r}
\specialrule{.1em}{0em}{.2em}
\specialrule{.1em}{0em}{1em}
    \Large \textsc{Learning} & 
    \framebox{$\Gamma \vdash \tau \rhd \chi \rightsquigarrow e$}\\
    & \\
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSp]acing{4pt}
    \def\defaultHypSeparation{\hskip .4in}
        \AxiomC{$\Gamma \vdash list \,\tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow [e_1,\dots,e_n]$}
        \AxiomC{$\bigwedge_{i=1}^n e_i =_\beta e_n$}
        \RightLabel{\textsc{(L-Wrld)}}
        \BinaryInfC{$\Gamma \vdash \tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow e$}
        

        \DisplayProof
    } \\\\
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .4in}
        \AxiomC{$\Gamma, \bigcup_{i=1}^n \chi_i{:}\tau \vdash list \,\tau \rightsquigarrow [e_1,\dots,e_n]$}
        \AxiomC{$\bigwedge_{i=1}^{n} e_i =_\beta \chi_i$}
            \RightLabel{\textsc{(L-Base)}}
        \BinaryInfC{$\Gamma \vdash list \,\tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow [e_1,\dots,e_n]$}
        

        \DisplayProof
    } \\\\
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .4in}
        \AxiomC{$\Gamma, \bigcup_{i=1}^n\pi_1(\chi_i){:}\tau_a \vdash list \,\tau_b \rhd [\pi_2(\chi_1),\dots,\pi_2(\chi_n)] \rightsquigarrow [e_1\pi_1(\chi_1),\dots,e_n\pi_1(\chi_n)]$}
        \RightLabel{\textsc{(L-EAbs)}}
        \UnaryInfC{$\Gamma \vdash list \,\tau_{a} \!\to\! \tau_b \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow [e_1,\dots,e_n]$}
        \DisplayProof
    } \\\\
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .4in}
        \AxiomC{$\Gamma, \alpha \vdash list \,\tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow [e_1\lceil\alpha\rceil,\dots,e_n\lceil\alpha\rceil]$}
        \RightLabel{\textsc{(L-ETAbs)}}
        \UnaryInfC{$\Gamma \vdash list \,\forall\alpha.\tau \rhd [\Lambda\alpha.\chi_1,\dots,\Lambda\alpha.\chi_n] \rightsquigarrow [e_1,\dots,e_n]$}
        \DisplayProof
    } \\\\
    \multicolumn{2}{c}{
    \def\extraVskip{1pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .4in}
    \alwaysNoLine
        \AxiomC{$\Gamma\vdash list \,\tau_{a} \!\to\! \tau_c \rhd [\chi_1,\dots,\chi_j] \rightsquigarrow [e_1,\dots,e_j]$}
        \UnaryInfC{$\Gamma\vdash list \,\tau_{b} \!\to\! \tau_c \rhd [\chi_1,\dots,\chi_k] \rightsquigarrow [e_1,\dots,e_k]$}
        \RightLabel{\textsc{(L-Sum)}}
        \alwaysSingleLine
        \def\extraVskip{4pt}
        \UnaryInfC{$\Gamma \vdash list \,(\tau_a\!+\!\tau_b) \!\to\! \tau_c \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow [e_1,\dots,e_n]$}
        \DisplayProof
    } \\
    & \\
\specialrule{.1em}{1em}{0em}
\end{tabular}
\caption{Learning from examples in System F}
    \label{fig:learning-examples}
\end{figure}

The informal process of learning described can be made formal via the relation presented in Figure \ref{fig:learning-examples}.

\textsc{(L-Wrld)} says that if you can learn a list of programs $[e_1,\dots,e_n]$, where $e_1,\dots,e_n$ are equivalent, then you can learn the program $e_1$. This rule is used to create $n$ worlds for $n$ examples at the start of learning.
\begin{prooftree}
\def\extraVskip{4pt}
\def\labelSpacing{4pt}
	\AxiomC{$\Gamma \vdash list\,nat\!\to\!nat \rhd [\langle1,1\rangle,\langle2,2\rangle] \rightsquigarrow [\lam x{:}nat.x,\lam x{:}nat.x]$}
	\RightLabel{\textsc{(L-Wrld)}}
	\UnaryInfC{$\Gamma \vdash nat\!\to\!nat \rhd [\langle1,1\rangle,\langle2,2\rangle] \rightsquigarrow \lam x{:}nat.x$}
\end{prooftree}

\textsc{(L-Base)} says that if you can learn a list of programs from its type and each $e_i$ is equivalent to some $\chi_i$ for $0\leq i\leq n$, then we can use each $\chi_i$ as an example output. For instance, $(\lam x{:}nat.x)1 =_\beta 1$. This means we can use $1$ as an example output for $(\lam x{:}nat.x)$. When learning, this rule is the "base" case. After exhausting the example information, this rule turns the learning process into learning from types.
\begin{prooftree}
\def\extraVskip{4pt}
\def\labelSpacing{4pt}
	\AxiomC{$\Gamma, 1{:}nat \vdash list\,nat \rightsquigarrow [(\lam x{:}nat.x)1]$}
	\AxiomC{$(\lam x{:}nat.x)1 =_\beta 1$}
	\RightLabel{\textsc{(L-Base)}}
	\BinaryInfC{$\Gamma \vdash list\,nat \rhd [\langle1\rangle] \rightsquigarrow [(\lam x{:}nat.x)1]$}
\end{prooftree}



\textsc{(L-EAbs)} says that if you can learn a list of applications $[e_1\pi_1(\chi_1),\dots,e_n\pi_1(\chi_n)]$, then you can learn a list of abstractions $[e_1,\dots,e_n]$ from examples where each $\pi_1\chi_i$ are inputs for $0 \leq i \leq n$. Note that $\pi_1$ is the first projection of an example tuple.
\begin{prooftree}
\def\extraVskip{4pt}
\def\labelSpacing{4pt}
	\AxiomC{$\Gamma, 1{:}nat, 2{:}nat \vdash list\,nat \rhd [\langle1\rangle,\langle2\rangle] \rightsquigarrow [(\lam x{:}nat.x)1,(\lam x{:}nat.x)2]$}
	\RightLabel{\textsc{(L-EAbs)}}
	\UnaryInfC{$\Gamma \vdash list\,nat\!\to\!nat \rhd [\langle1,1\rangle,\langle2,2\rangle] \rightsquigarrow [\lam x{:}nat.x,\lam x{:}nat.x]$}
\end{prooftree}

\textsc{(L-ETAbs)} says that if you can learn a list of applications $[e_1\lceil\alpha\rceil,\dots,e_n\lceil\alpha\rceil]$, then you can learn a list of polymorphic abstractions $[e_1,\dots,e_n]$ from examples where each $\Lambda\alpha.\chi_i$ are inputs for $0 \leq i \leq n$.
\begin{prooftree}
\def\extraVskip{4pt}
\def\labelSpacing{4pt}
	\AxiomC{$\Gamma, \alpha \vdash list \,\alpha\!\to\!\alpha \rhd [\langle z,z\rangle] \rightsquigarrow [(\Lambda\alpha.\lam x{:}\alpha.x)\lceil\alpha\rceil]$}
	\RightLabel{\textsc{(L-TAbs)}}
	\UnaryInfC{$\Gamma \vdash list \,\forall\alpha.\alpha\!\to\!\alpha \rhd [\Lambda\alpha.\langle z,z\rangle] \rightsquigarrow [\Lambda\alpha.\lam x{:}\alpha.x]$}
\end{prooftree}

\textsc{(L-Sum)} says that if you can learn a list of programs whose input is type $\tau_a$ and another list of programs whose input is type $\tau_b$, then you can learn a list of program whose input is the sum type $\tau_a\!+\!\tau_b$ and whose examples contain inputs of both type $\tau_a$ and type $\tau_b$. During learning, this rule is perhaps the most useful. It lets you distribute examples when encountering sum types as inputs, which ends up creating two sub-problems, each of which dealing with a smaller set of examples, which are easier to satisfy. In the example, let $e \equiv case(b)\,of\, \iota_1(true) \mapsto false \mid \iota_2(false) \mapsto true$
\begin{prooftree}
    \def\extraVskip{1pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .4in}
    \alwaysNoLine
        \AxiomC{$\Gamma\vdash list \,bool \!\to\! bool \rhd [\langle true, false\rangle] \rightsquigarrow [\lam x{:}bool.false]$}
        \UnaryInfC{$\Gamma\vdash list \,bool \!\to\! bool \rhd [\langle false, true\rangle] \rightsquigarrow [\lam y{:}bool.true]$}
        \RightLabel{\textsc{(L-Sum)}}
        \alwaysSingleLine
        \def\extraVskip{4pt}
        \UnaryInfC{$\Gamma \vdash list \,(bool\!+\!bool) \!\to\! bool \rhd [\langle true,false \rangle, \langle false, true\rangle] \rightsquigarrow [e,e]$}
\end{prooftree}

This new relation extends learning from types. Now System F can learn from examples too. So far, our presentation of learning is both sound and complete. You can learn every program in System F from types. We now show that these results hold when learning from examples too.

\section{Metatheory}
Because our aim is to show every program in System F is learnable from examples, we want to show that learning is still equivalent to typing. As with learning from types, we show both completeness and soundness of learning with respect to typing---giving us the equivalence. These proofs entail a bit more work, but are far simpler than similar presentations of metatheory for languages which permit learning from examples, e.g. \textsc{Myth} \cite{osera2015program}. The mathematical convenience is afforded by not introducing any machinery into System F for learning. 

\subsection{Typing and Learning are still equivalent}

To show completeness, we need to show that for any program in System F there exists a list of examples from which it can be learned. This turns out to be trivial. Hence, a stronger statement we want is that we can learn any program which is satisfied by a list of examples. It would be problematic if we could learn any program in System F, but only from a particular subset of the examples which describe that program. Showing both of these gives strong guarantees on learning. If a list of examples describes a program, you can learn that program---and this list exists for every program in System F.

\begin{lemma}[\textsc{Completeness$_a$ of Learning}]
If $\,\Gamma \vdash e : \tau$ then there exist some $[\chi_1,\dots,\chi_n]$ such that $\,\Gamma \vdash \tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow e$
\label{completeness-learning-examples-a}
\end{lemma}
\begin{proof}
For an arbitrary program $e$ of type $\tau$, let $[\langle e\rangle]$ constitute its example list. This example list has no input, only an ouput $e$. Hence the statemement $\,\Gamma \vdash \tau \rhd [\langle e\rangle] \rightsquigarrow e$ asks whether we can learn a program $e$ of type $\tau$ whose output is $e$. 

Now, note that for any program $e =_\beta e$ by definition of the reflexive, transitive, and symmetric evaluation relation. Additionally, for any program $e$ of type $\tau$, a list $[e]$ can be learned from type $list\,\tau$. Because learning from types is complete, this works for any program.

Knowing this, we can apply the following learning rules:
\begin{prooftree}
\def\extraVskip{4pt}
\def\labelSpacing{4pt}
	\AxiomC{$\,\Gamma,x{:}\tau \vdash list\,\tau \rightsquigarrow [e]$}
	\AxiomC{$e=_\beta e$}
	\RightLabel{\textsc{(L-Base)}}
	\BinaryInfC{$\,\Gamma \vdash list\,\tau \rhd [\langle e\rangle] \rightsquigarrow [e]$}
	\AxiomC{$e=_\beta e$}
	\RightLabel{\textsc{(L-Wrld)}}
	\BinaryInfC{$\,\Gamma \vdash \tau \rhd [\langle e\rangle] \rightsquigarrow e$}
\end{prooftree}

Hence for any program $e$ in System F, it can be learned from examples when the example is $[\langle e\rangle]$.
\end{proof}

This is nice, but the less interesting completeness result. We want to make sure that a list of examples which describes a program can be used to learn it. An example list satisfies (or describes) a program, where if the program is applied to the inputs, the corresponding outputs are equivalent.

\begin{lemma}[\textsc{Completeness$_b$ of Learning}]
If $\,\Gamma \vdash e : \tau$ and there exist some $[\chi_1,\dots,\chi_n]$ which satisfies $e$, then $\,\Gamma \vdash \tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow e$.
\label{completeness-learning-examples-b}
\end{lemma}
\begin{proof}
There are two general cases to prove, when $e$ has inputs and when $e$ has no inputs. For programs without input, see previous lemma. For programs with input let $\Gamma \vdash e : \tau$, where $[\chi_1,\dots,\chi_n]$ are examples which satisfy $e$. Because of Theorem \ref{equivalence-learning}, we also know $\Gamma \vdash \tau \rightsquigarrow e$. Because lists are learnable, we also know $\Gamma \vdash list\,\tau \rightsquigarrow [e_1,\dots,e_n]$, where $\bigwedge_{i=1}^n e_i =_\beta e_n$.
\vspace{.3em}

Now let's deconstruct each $\chi_i$ into its input and output components: $\langle\chi^{in}_i, \chi^{out}_i\rangle$. Since each $\chi_i$ satisfies $e$, it must be that $ \bigwedge_{i=1}^n e_i\chi_i^{in} =_\beta \chi_i^{out}$. Due to satisfaction, $e_i\chi_i^{in}$ is guaranteed to be well-typed. Hence $\Gamma,\bigcup_{i=1}^n\chi_i^{out}{:}\tau^{out}, \bigcup_{i=1}^n\chi_i^{in}{:}\tau^{in} \vdash list\,\tau^{out} \rightsquigarrow [e_1\chi_i^{in},\dots,e_n\chi_n^{in}]$. With this, we can apply the following rules:

\begin{prooftree}
\def\extraVskip{4pt}
\def\labelSpacing{4pt}
	\AxiomC{$\Gamma,\bigcup_{i=1}^n\chi_i^{out}{:}\tau^{out}, \bigcup_{i=1}^n\chi_i^{in}{:}\tau^{in} \vdash list\,\tau^{out} \rightsquigarrow [e_1\chi_i^{in},\dots,e_n\chi_n^{in}]$}
	\AxiomC{$\bigwedge_{i=1}^n e_i\chi_i^{in} =_\beta \chi_i^{out}$}
	\RightLabel{\textsc{(L-Base)}}
	\BinaryInfC{$\Gamma,  \bigcup_{i=1}^n\chi_i^{in}{:}\tau^{in} \vdash list\,\tau^{out} \rhd [\langle\chi_1^{out}\rangle,\dots,\langle\chi_n^{out}\rangle] \rightsquigarrow [e_1\chi_i^{in},\dots,e_n\chi_n^{in}]$}
	\RightLabel{\textsc{(L-EAbs)}}
	\UnaryInfC{$\Gamma \vdash list\,\tau \rhd [\langle\chi_1^{in},\chi_1^{out}\rangle,\dots,\langle\chi_n^{in},\chi_n^{out}\rangle] \rightsquigarrow [e_1,\dots,e_n]$}
\end{prooftree}
Remembering that $\chi_i \equiv \langle\chi_i^{in},\chi_i^{out}\rangle$ and that $\bigwedge_{i=1}^n e_i =_\beta e_n$, we finally prove the necessary result.
\begin{prooftree}
\def\extraVskip{4pt}
\def\labelSpacing{4pt}
	\AxiomC{$\Gamma \vdash list\,\tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow [e_1,\dots,e_n]$}
	\AxiomC{$\bigwedge_{i=1}^n e_i =_\beta e_n$}
	\RightLabel{\textsc{(L-Wrld)}}
	\BinaryInfC{$\Gamma \vdash \tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow e$}
\end{prooftree}

Note that if there are $n$ inputs to the examples which satisfy $e$, then \textsc{(L-EAbs)} must be applied $n$ times to fully reconstruct the examples.
\end{proof}

\begin{lemma}[\textsc{Soundness of Learning}]
If $\,\Gamma \vdash \tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow e$ then $\Gamma \vdash e : \tau$
\label{soundness-learning-examples}
\end{lemma}
\begin{proof}
Case analysis on learning rules.

\underline{Case 1}: \textsc{(L-Wrld)}\\
We know that we can learn from examples a list of length $n$ where each entry is $e$.
\begin{prooftree}
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .4in}
        \AxiomC{$\Gamma \vdash list \,\tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow [e_1,\dots,e_n]$}
        \AxiomC{$\bigwedge_{i=1}^n e_i =_\beta e_n$}
        \RightLabel{\textsc{(L-Wrld)}}
        \BinaryInfC{$\Gamma \vdash \tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow e$}
\end{prooftree}
After applying \textsc{(L-EAbs)} and \textsc{(L-Base)} it must be that $\Gamma \vdash list \,\tau' \rightsquigarrow [e_1\chi_1^{in},\dots,e_n\chi_n^{in}]$. The list is only learnable if each element is learnable, hence $\Gamma \vdash \tau' \rightsquigarrow e_i\chi_i^{in}$. And an application is only learnable if each side of the application is learnable, hence $\Gamma \vdash \tau \rightsquigarrow e$ (noting $e_i =_\beta e$). Because of the equivalence of learning from types and typing, we have $\Gamma \vdash e : \tau$. 

\underline{Case 2}: \textsc{(L-Base)}\\
We know we can learn learn from types a list of length $n$ where each entry is $e$.

\begin{prooftree}
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .4in}
        \AxiomC{$\Gamma, \bigcup_{i=1}^n \chi_i{:}\tau \vdash list \,\tau \rightsquigarrow [e_1,\dots,e_n]$}
        \AxiomC{$\bigwedge_{i=1}^{n} e_i =_\beta \chi_i$}
            \RightLabel{\textsc{(L-Base)}}
        \BinaryInfC{$\Gamma \vdash list \,\tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow [e_1,\dots,e_n]$}
\end{prooftree}

The list is only learnable if each element is learnable, hence $\Gamma \vdash \tau \rightsquigarrow e$. Because of the equivalence of learning from types and typing, we have $\Gamma \vdash e : \tau$. 

\underline{Case 3}: \textsc{(L-EAbs)}\\
We know we can learn learn from types a list of length $n$ and type $\tau_b$ where each entry is $e\pi_1(\chi_i)$.

\begin{prooftree}
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .4in}
        \AxiomC{$\Gamma, \bigcup_{i=1}^n\pi_1(\chi_i){:}\tau_a \vdash list \,\tau_b \rhd [\pi_2(\chi_1),\dots,\pi_2(\chi_n)] \rightsquigarrow [e_1\pi_1(\chi_1),\dots,e_n\pi_1(\chi_n)]$}
        \RightLabel{\textsc{(L-EAbs)}}
        \UnaryInfC{$\Gamma \vdash list \,\tau_{a} \!\to\! \tau_b \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow [e_1,\dots,e_n]$}
\end{prooftree}

After applying \textsc{(L-Base)} it must be that $\Gamma, \bigcup_{i=1}^n\pi_1(\chi_i){:}\tau_a \vdash list \,\tau_b \rightsquigarrow [e_1\pi_1(\chi_1),\dots,e_n\pi_1(\chi_n)]$. The list is only learnable if each element is learnable, hence $\Gamma, \bigcup_{i=1}^n\pi_1(\chi_i){:}\tau_a \vdash \tau_b \rightsquigarrow e_1\pi_1(\chi_1)$. And an application is only learnable if each side of the application is learnable, hence $\Gamma \vdash \tau_a\!\to\!\tau_b \rightsquigarrow e$ (noting $e_i =_\beta e$). Because of the equivalence of learning from types and typing, we have $\Gamma \vdash e : \tau_a\!\to\!\tau_b$. 

\underline{Case 4}: \textsc{(L-ETAbs)}\\
Same strategy as Case 3, except using type application. 

\underline{Case 5}: \textsc{(L-Sum)}\\
After assuming \textsc{(L-Sum)}, reduces to proof of Case 3.
\end{proof}
\vspace{-1.0em}

\begin{theorem}[\textsc{Equivalence of Typing and Learning}]
If and only if $\,\Gamma \vdash \tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow e$, then $\Gamma \vdash e : \tau$ and $[\chi_1,\dots,\chi_n]$ satisfies $e$.
\label{equivalence-learning-examples}
\end{theorem}
\begin{proof}
Directly from Lemmas \ref{completeness-learning-examples-a}, \ref{completeness-learning-examples-b} and \ref{soundness-learning-examples}.
\end{proof}
\vspace{-.8em}

Because we can only learn a program if and only if it is well typed, it follows that learned programs obey progress, preservation, and normalization. Each proof invokes the equivalence theorem between typing and learning, and then the respective progress, preservation, and normalization theorems for typing.

\subsection{Learned programs still don't get stuck}

\begin{corollary}[\textsc{Progress in Learning}]
If $e$ is a learned program, then either $e$ is a value or else there is some program $e'$ such that $e \to_\beta e'$.
\label{progress-learning}
\end{corollary}
\begin{proof}
Directly from Theorems \ref{equivalence-learning-examples} and \ref{progress-typing}.
\end{proof}
\vspace{-.8em}

We shouldn't be able to learn programs which get stuck during evaluation, same as with typing. If I learn a program, either its a value or it can be evaluated to another program. When learning from examples, learning still obeys progress.

\subsection{Learned programs still don't change type}

\begin{corollary}[\textsc{Preservation in Learning}]
If $\,\Gamma \vdash \tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow e$ and $e \to_\beta e'$, then $\,\Gamma \vdash \tau \rhd [\chi_1,\dots,\chi_n] \rightsquigarrow e'$.
\label{preservation-learning}
\end{corollary}
\begin{proof}
Directly from Theorems \ref{equivalence-learning-examples} and \ref{preservation-typing}. 
\end{proof}
\vspace{-.8em}

We shouldn't be able to learn programs of a different type than the one provided. If I learn a program, and it evaluates to another program, then I should be able to learn that new program from the same type. When learning from examples, learning still obeys preservation.

\subsection{Learned programs still always halt}

\begin{corollary}[\textsc{Normalization in Evaluation}]
Learned programs in System F always evaluate to a value, to a normal form.
\label{normalization-learning}
\end{corollary}
\begin{proof}
Directly from Theorems \ref{equivalence-learning-examples} and \ref{normalization-evaluation}. 
\end{proof}
\vspace{-.8em}

We shouldn't be able to learn programs which never finish computing. They must halt. As with learning from types, learning from examples only lets you learn halting programs.

\section{Learning identity, not, and successor}

For examples, any program in System F can serve as input or output. There are no restrictions. In similar works which allow learning from examples, like \textsc{Myth} \cite{osera2015program}, there are restrictions on what examples can look like. Namely, functions cannot appear as output in an example. This makes it impossible to learn many higher-order programs. In fact, the motivation for this work started from observing this limitation in \textsc{Myth}. It made it impossible to learn compilers, higher-order programs which specify programming languages. 

Here we show how to learn several programs from examples before discussing further the prospect of learning not only programs, but programming languages.

\subsection{Learning polymorphic identity}

\begin{lemma}[\textsc{Polymorphic identity is learnable}] $\cdot \vdash \forall\alpha.\alpha \!\to\! \alpha \rhd [\Lambda\alpha.\langle z,z\rangle] \rightsquigarrow \Lambda\alpha.\lam x{:}\alpha.x$
\label{learning-poly-identity}
\end{lemma}
\begin{proof}

\begin{enumerate}[label=\textit{(\roman*)}]

\item $\alpha, z{:}\alpha \vdash \alpha\!\to\!\alpha \rightsquigarrow (\Lambda\alpha.\lam x{:}\alpha.x)\lceil\alpha\rceil$
\begin{prooftree}
\def\extraVskip{4pt}
\def\labelSpacing{4pt}
    \AxiomC{$x{:}nat \in \alpha, z{:}\alpha, x{:}\alpha$}
    \RightLabel{\textsc{(L-Var)}}
    \UnaryInfC{$\alpha, z{:}\alpha, x{:}\alpha \vdash \alpha \rightsquigarrow x$}
    \RightLabel{\textsc{(L-Abs)}}
    \UnaryInfC{$\alpha, z{:}\alpha \vdash \alpha\!\to\!\alpha \rightsquigarrow \lam x{:}\alpha.x$}
    \RightLabel{\textsc{(L-TAbs)}}
    \UnaryInfC{$\alpha, z{:}\alpha \vdash \forall\alpha.\alpha\!\to\!\alpha \rightsquigarrow \Lambda\alpha.\lam x{:}\alpha.x$}
    \RightLabel{\textsc{(L-Abs)}}
    \AxiomC{$\alpha, z{:}\alpha \vdash \alpha \rightsquigarrow \alpha$}
    \RightLabel{\textsc{(L-App)}}
    \BinaryInfC{$\alpha, z{:}\alpha \vdash \alpha\!\to\!\alpha \rightsquigarrow (\Lambda\alpha.\lam x{:}\alpha.x)\lceil\alpha\rceil$}
    \alwaysNoLine
    \UnaryInfC{}
\end{prooftree}

\item $\alpha, z{:}\alpha \vdash \alpha \rightsquigarrow (\Lambda\alpha.\lam x{:}\alpha.x)\lceil\alpha\rceil z$
\begin{prooftree}
\def\extraVskip{4pt}
\def\labelSpacing{4pt}
    \AxiomC{\textit{(i)}}
    \UnaryInfC{$\alpha, z{:}\alpha \vdash \alpha\!\to\!\alpha \rightsquigarrow (\Lambda\alpha.\lam x{:}\alpha.x)\lceil\alpha\rceil$}
    \AxiomC{$z{:}\alpha \in \alpha, z{:}\alpha$}
    \RightLabel{\textsc{(L-Var)}}
    \UnaryInfC{$\alpha, z{:}\alpha \vdash \alpha \rightsquigarrow z$}
    \RightLabel{\textsc{(L-App)}}
    \BinaryInfC{$\alpha, z{:}\alpha \vdash \alpha \rightsquigarrow (\Lambda\alpha.\lam x{:}\alpha.x)\lceil\alpha\rceil z$}
    \alwaysNoLine
    \UnaryInfC{}
\end{prooftree}

\item $\alpha, z{:}\alpha \vdash list\,\alpha \rightsquigarrow [(\Lambda\alpha.\lam x{:}\alpha.x)\lceil\alpha\rceil z]$
\begin{prooftree}
\def\extraVskip{4pt}
\def\labelSpacing{4pt}
    \AxiomC{\textit{(ii)}}
    \UnaryInfC{$\alpha, z{:}\alpha \vdash \alpha \rightsquigarrow (\Lambda\alpha.\lam x{:}\alpha.x)\lceil\alpha\rceil z$}
    \AxiomC{$\alpha, z{:}\alpha \vdash list \,\alpha \rightsquigarrow [\,]$}
    \RightLabel{\textsc{(L-Cons)}}
    \BinaryInfC{$\alpha, z{:}\alpha \vdash list\,\alpha \rightsquigarrow [(\Lambda\alpha.\lam x{:}\alpha.x)\lceil\alpha\rceil z]$}
    \alwaysNoLine
    \UnaryInfC{}
\end{prooftree}

\item $\cdot \vdash \forall\alpha.\alpha \!\to\! \alpha \rhd [\Lambda\alpha.\langle z,z\rangle] \rightsquigarrow \Lambda\alpha.\lam x{:}\alpha.x$
\begin{prooftree}
\def\extraVskip{4pt}
\def\labelSpacing{4pt}
    \AxiomC{\textit{(iii)}}
    \UnaryInfC{$\alpha, z{:}\alpha \vdash list\,\alpha \rightsquigarrow [(\Lambda\alpha.\lam x{:}\alpha.x)\lceil\alpha\rceil z]$}
    \AxiomC{$(\Lambda\alpha.\lam x{:}\alpha.x)\lceil\alpha\rceil z =_\beta z$}
    \RightLabel{\textsc{(L-Base)}}
    \alwaysSingleLine
    \def\extraVskip{4pt}
    \BinaryInfC{$\alpha, z{:}\alpha \vdash list\,\alpha \rhd [\langle z\rangle] \rightsquigarrow [(\Lambda\alpha.\lam x{:}\alpha.x)\lceil\alpha\rceil z]$}
    \RightLabel{\textsc{(L-EAbs)}}
    \UnaryInfC{$\alpha \vdash list\,\alpha \!\to\! \alpha \rhd [\langle z,z\rangle] \rightsquigarrow [(\Lambda\alpha.\lam x{:}\alpha.x)\lceil\alpha\rceil]$}
    \RightLabel{\textsc{(L-ETAbs)}}
    \UnaryInfC{$\cdot \vdash list\,\forall\alpha.\alpha \!\to\! \alpha \rhd [\Lambda\alpha.\langle z,z\rangle] \rightsquigarrow [\Lambda\alpha.\lam x{:}\alpha.x]$}
    \RightLabel{\textsc{(L-Wrld)}}
    \UnaryInfC{$\cdot \vdash \forall\alpha.\alpha \!\to\! \alpha \rhd [\Lambda\alpha.\langle z,z\rangle] \rightsquigarrow \Lambda\alpha.\lam x{:}\alpha.x$}
    \alwaysNoLine
    \UnaryInfC{}
\end{prooftree}

\end{enumerate}
\end{proof}

\subsection{Learning boolean not}
\begin{lemma}[\textsc{Not is learnable}] \begin{singlespace}$\cdot \vdash (bool\!+\!bool)\!\to\!bool \rhd [\langle true,false \rangle, \langle false, true\rangle] \rightsquigarrow e$

\textsc{Note:} $e \equiv case(b)\,of\, \iota_1(true) \mapsto false \mid \iota_2(false) \mapsto true$. Additionally, that $true$ and $false$ are learnable from any context. \end{singlespace}
\label{learning-boolean-not}
\end{lemma}
\begin{proof}

\begin{enumerate}[label=\textit{(\roman*)}]

\item $true{:}bool \vdash bool \rightsquigarrow (\lam x{:}bool.false)true$
\begin{prooftree}
    \alwaysSingleLine
    \def\extraVskip{4pt}
    		\AxiomC{$true{:}bool, x{:}bool \vdash bool \rightsquigarrow false$}
    		\RightLabel{\textsc{(L-Abs)}}
        \UnaryInfC{$true{:}bool \vdash bool \rightsquigarrow \lam x{:}bool.false$}
        \AxiomC{$true{:}bool \in true{:}bool$}
        \RightLabel{\textsc{(L-Var)}}
        \UnaryInfC{$true{:}bool \vdash bool \rightsquigarrow true$}
        \RightLabel{\textsc{(L-App)}}
        \BinaryInfC{$true{:}bool \vdash bool \rightsquigarrow (\lam x{:}bool.false)true$}

		
\end{prooftree}


\item $\cdot\vdash list \,bool \!\to\! bool \rhd [\langle true, false\rangle] \rightsquigarrow [\lam x{:}bool.false]$
\begin{prooftree}
    \alwaysSingleLine
    \def\extraVskip{4pt}
    		\AxiomC{\textit{(i)}}
    		\AxiomC{$true{:}bool \vdash list\,bool \rightsquigarrow []$}
    		\RightLabel{\textsc{(L-Cons)}}
        \BinaryInfC{$true{:}bool \vdash list\,bool \rightsquigarrow [(\lam x{:}bool.false)true]$}
        \AxiomC{$(\lam x{:}bool.false)true =_\beta false$}
        \RightLabel{\textsc{(L-Sum)}}
        \BinaryInfC{$true{:}bool \vdash list bool \rhd [\langle false\rangle] \rightsquigarrow [(\lam x{:}bool.false)true]$}
        \RightLabel{\textsc{(L-EAbs)}}
		\UnaryInfC{$\Gamma\vdash list \,bool \!\to\! bool \rhd [\langle true, false\rangle] \rightsquigarrow [\lam x{:}bool.false]$}
\end{prooftree}


\item $false{:}bool \vdash list\,bool \rightsquigarrow (\lam y{:}bool.true)false$
\begin{prooftree}
    \alwaysSingleLine
    \def\extraVskip{4pt}
    		\AxiomC{$false{:}bool, y{:}bool \vdash bool \rightsquigarrow true$}
    		\RightLabel{\textsc{(L-Abs)}}
        \UnaryInfC{$false{:}bool \vdash bool \rightsquigarrow \lam y{:}bool.true$}
        \AxiomC{$false{:}bool \in false{:}bool$}
        \RightLabel{\textsc{(L-Var)}}
        \UnaryInfC{$false{:}bool \vdash bool \rightsquigarrow false$}
        \RightLabel{\textsc{(L-App)}}
        \BinaryInfC{$false{:}bool \vdash bool \rightsquigarrow (\lam y{:}bool.true)false$}

		
\end{prooftree}


\item $\cdot\vdash list \,bool \!\to\! bool \rhd [\langle false, true\rangle] \rightsquigarrow [\lam y{:}bool.true]$
\begin{prooftree}
    \alwaysSingleLine
    \def\extraVskip{4pt}
    		\AxiomC{\textit{(iii)}}
    		\AxiomC{$false{:}bool \vdash list\,bool \rightsquigarrow []$}
    		\RightLabel{\textsc{(L-Cons)}}
        \BinaryInfC{$false{:}bool \vdash list\,bool \rightsquigarrow [(\lam y{:}bool.true)false]$}
        \AxiomC{$(\lam y{:}bool.true)false =_\beta true$}
        \RightLabel{\textsc{(L-Sum)}}
        \BinaryInfC{$false{:}bool \vdash list bool \rhd [\langle true\rangle] \rightsquigarrow [(\lam y{:}bool.true)false]$}
        \RightLabel{\textsc{(L-EAbs)}}
		\UnaryInfC{$\cdot\vdash list \,bool \!\to\! bool \rhd [\langle false, true\rangle] \rightsquigarrow [\lam y{:}bool.true]$}
\end{prooftree}


\item $\cdot \vdash (bool\!+\!bool)\!\to\!bool \rhd [\langle true,false \rangle, \langle false, true\rangle] \rightsquigarrow e$
\begin{prooftree}
    \alwaysSingleLine
    \def\extraVskip{4pt}
        \AxiomC{\textit{(ii)}}
        \AxiomC{\textit{(iii)}}
        \RightLabel{\textsc{(L-Sum)}}
        \BinaryInfC{$\cdot \vdash list \,(bool\!+\!bool) \!\to\! bool \rhd [\langle true,false \rangle, \langle false, true\rangle] \rightsquigarrow [e,e]$}
        \RightLabel{\textsc{(L-Wrld)}}
		\UnaryInfC{$\cdot \vdash (bool\!+\!bool)\!\to\!bool \rhd [\langle true,false \rangle, \langle false, true\rangle] \rightsquigarrow e$}
\end{prooftree}
\end{enumerate}
\end{proof}


\subsection{Learning church successor}

\begin{lemma}[\textsc{Successor is learnable}] \begin{singlespace} Let $church \equiv \forall\alpha.(\alpha\!\to\!\alpha)\!\to\!\alpha\!\to\!\alpha$, $\bar{0} \equiv \Lambda\alpha.\lam f{:}\alpha\!\to\!\alpha.\lam x{:}\alpha.x$, $\bar{1} \equiv \Lambda\alpha.\lam f{:}\alpha\!\to\!\alpha.\lam x{:}\alpha.fx$, and $succ \equiv \lam n{:}church.\Lambda\alpha.\lam f{:}\alpha\!\to\!\alpha.\lam x{:}\alpha.f(n \lceil\alpha\rceil f x)$. And assume that $\bar{0}$, $\bar{1}$, and $succ$ can be learned from any context.

Show $\cdot \vdash church\!\to\!church \rhd [\langle\bar{0},\bar{1}\rangle] \rightsquigarrow succ$.\end{singlespace}
\label{learning-succ}
\end{lemma}

\begin{proof}

\begin{enumerate}[label=\textit{(\roman*)}]

\item $\bar{0}{:}church \vdash church\!\to\!church \rightsquigarrow succ$

\textsc{Note:} Can be learned from any context.

\item $\cdot \vdash church\!\to\!church \rhd [\langle\bar{0},\bar{1}\rangle] \rightsquigarrow succ$
\begin{prooftree}
    \alwaysSingleLine
    \def\extraVskip{4pt}
    		\AxiomC{\textit{(i)}}
    		\AxiomC{$\bar{0}{:}church \in \bar{0}{:}church$}
    		\UnaryInfC{$\bar{0}{:}church \vdash church \rightsquigarrow \bar{0}$}
    		\BinaryInfC{$\bar{0}{:}church \vdash church \rightsquigarrow (succ)\bar{0}$}
    		\AxiomC{$\bar{0}{:}church \vdash list\,church \rightsquigarrow []$}
    		\BinaryInfC{$\bar{0}{:}church \vdash list\,church \rightsquigarrow [(succ)\bar{0}]$}
    		\AxiomC{$(succ)\bar{0} =_\beta \bar{1}$}
    		\RightLabel{\textsc{(L-Base)}}
        \BinaryInfC{$\cdot \vdash list\,church \rhd [\langle\bar{1}\rangle] \rightsquigarrow [(succ)\bar{0}]$}
        \RightLabel{\textsc{(L-EAbs)}}
        \UnaryInfC{$\cdot \vdash list\,church\!\to\!church \rhd [\langle\bar{0},\bar{1}\rangle] \rightsquigarrow [succ]$}
        \RightLabel{\textsc{(L-Wrld)}}
        \UnaryInfC{$\cdot \vdash church\!\to\!church \rhd [\langle\bar{0},\bar{1}\rangle] \rightsquigarrow succ$}
\end{prooftree}


\end{enumerate}
\end{proof}

\section{Learning from Examples, Practically}
% present the augmented relation
% state completeness/soundness result

%\section{Implementation}

We have implemented a proof-of-concept prototype, with promising results.
(Reviewers: see the artifact attached.)

% how to make combinatorial search easier
% - enumerating normal form programs
% - deducing operationally distinct type applications
% - memoizing recurisve calls?
% - algebraic data types let us use examples productively
%    - sum types split examples
%	 - product types generate subproblems

%\section{Experiments}

The implementation of learning from types is in the function {\tt genTerms} in {\tt learning.hs}. All the code in this section is in Haskell and runs in {\tt ghci}:
\begin{verbatim}
genTerms TyBool [] 5
\end{verbatim}

That generates all terms of type {\tt Bool} from the empty context, up to an AST size 5.

The implementation of learning from examples is in the function {\tt lrnTerms} in {\tt learning.hs}.
\begin{verbatim}
lrnTerms (TyAbs TyBool TyBool) [InTm TmTrue (Out TmTrue)] [] [] 3
\end{verbatim}

That generates all terms of type {\tt Bool->Bool} from the empty context, up to an AST size 3 \emph{and} which satisfy the example $<tt,tt>$. 

To generate polymorphic terms, our examples include types. These types are used to instantiate an example at a particular base type. For example, run the following in ghci to learn at type $(\forall X.X->X)$ with examples $<Bool,tt,tt>$:
\begin{verbatim}
lrnTerms (TyTAbs "X" (TyAbs (TyVar "X") (TyVar "X")))
         [InTy TyBool (InTm TmTrue (Out TmTrue))]
         [] [] 4
\end{verbatim}
This will produce the polymorphic identity function.
% TODO show the production

The implementation ``works'' minus a programs which require multiple type applications. There's also a bottleneck in performance that becomes apparent when synthesizing programs at around AST depth 20, because of the way the type application rule is currently implemented for learning.

\section{Related Work}

The literature on synthesis is both vast and rapidly expanding. Therefore, we inevitably cherry-pick among approaches that have inspired us while covering a varied landscape as well.

\subsection{Type-driven synthesis}

The seminal works on type-driven synthesis by \citet{osera2015program}. demonstrates that you can have none-``magical'' approaches to synthesis: everything is predictable, such as for instance, needing trace-complete examples. We take inspiration from this work, and suggest a way forward for examples with higher-order functions in the input/output exammples. We do not require trace completeness for System F is strongly normalizing.

\citet{polikarpova2016program} have extended the typed-driven approaach to refinement types. This is interesting because refinement types, such as Liquid Types~\cite{rondon2012liquid} are rather expressive and the types can act as rich specification. The idea of liquid types has been researched in many languages beyond the initial ML-style setting: for example, Javascript~\cite{chugh2012nested} and Haskell~\cite{liquidhaskell}. Our work extends the type-driven approach in an orthogonal direction.

\subsection{Other approaches to synthesis}

Program Sketching~\cite{solar2008program} has been a promising approach to synthesis. It relies on specification and holes. We rely on types instead of full-blown specifications.

Microsoft Prose~\cite{msprose} has enabled the program demonstration of
Excel. The Microsoft team has packaged the lessons learned from
FlashFill~\cite{gulwani2011automating}  into a generic framework Flash Meta~\cite{flashmeta}. The key
insight is that witness functions (also known as reverse semantics)
are enough to speed up synthesis a for a domain-specific language. In
Prose, the synthesizer author defines a grammar for the language, the
semantics and the inverse semantics, and the framework automagically
creates the synthesizer.

Rosette~\cite{rosette} is a solver-aided language framework that similarly enables a separation of concern between domain and generic solving, based on satisfiability modulo theory underneath the hood. In Rosette, one can write a vanilla interpreter and get a synthesizer.

Another term for synthesis used in the learning community is program induction.
Differentiable programming such as for Forth~\cite{dforth} and Datalog~\cite{ddatalog} use relaxation techniques to go from symbol to neural.
Neural-Guided Search~\cite{webyrd-nips,dreamcoder} uses neural networks \emph{not} for creating programs but for guiding the search on the symbolic possibilities.

Inductive logic programming~\cite{ilp} provides a classical avenue for synthesis in Prolog and related programming languages. An exciting recent development is the work of~\citet{cropper} in which programs are learned through failures.


\section{Conclusion}

We offer a promising route to type-driven synthesis through learning in System F. Our main insight: System F provides a fruitful ground for synthesis thanks to parametricity and normalization. In addition to a declarative theory, we developed a small implementation to testbed our ideas. For future work, we intend to deveop an algorithmic formulation to expand the implementation and align it with the theory.

%% Acknowledgments
%%\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
%%  This material is based upon work supported by the
%%  \grantsponsor{GS100000001}{National Science
%%    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%%  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%%  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%%  conclusions or recommendations expressed in this material are those
%%  of the author and do not necessarily reflect the views of the
%%  National Science Foundation.
%%\end{acks}


%% Bibliography
\bibliography{refs.bib}

\newpage
%% Appendix
%%\appendix
\section{Appendix}
\subsection{Specification of System F}
\begin{figure}[h]
\centering
\setlength{\tabcolsep}{12pt}
\begin{tabular}{l  r}
\specialrule{.1em}{0em}{.2em}

\specialrule{.1em}{0em}{1em}
    \Large \textsc{Syntax} & \\
    & \\
    \begin{math}
    \setlength{\jot}{-2pt}
    \begin{aligned}
    e ::= \;& && \hspace*{.25in} \textsc{terms:}\\
        & x && \hspace*{.25in} \textit{variable}\\
        & e_1e_2 && \hspace*{.25in} \textit{application}\\
        & \lam x {:} \tau.e && \hspace*{.25in} \textit{abstraction}\\
        & e\lceil\tau\rceil && \hspace*{.25in} \textit{type application}\\    
        & \Lambda\alpha.e && \hspace*{.25in} \textit{type abstraction}\\
    \\
    v ::= \;& && \hspace*{.25in} \textsc{values:} \\
        & \lam x {:}\tau.e && \hspace*{.25in} \textit{abstraction}\\
        & \Lambda\alpha.e && \hspace*{.25in} \textit{type abstraction}\\
    \end{aligned}
    \end{math} & 
    \begin{math}
    \setlength{\jot}{-2pt}
    \begin{aligned}
    \tau ::= \;& && \hspace*{.25in} \textsc{types:}\\
        & \tau_1 \to \tau_2 && \hspace*{.25in} \textit{function type}\\
        & \forall\alpha.\tau && \hspace*{.25in} \textit{polymorphic type}\\
        & \alpha && \hspace*{.25in} \textit{type variable}\\
    \\
    \Gamma ::= \;& && \hspace*{.25in} \textsc{contexts:}\\
        & \cdot && \hspace*{.25in} \textit{empty}\\
        & x{:}\tau,\Gamma && \hspace*{.25in} \textit{variable}\\
        & \alpha,\Gamma && \hspace*{.25in} \textit{type variable}
    \end{aligned}
    \end{math}\\
    &\\
\specialrule{.1em}{1em}{0em}
\end{tabular}
\caption{Syntax in System F}
    \label{fig:syntax}
\end{figure}

\begin{figure}[h]
\centering
\setlength{\tabcolsep}{12pt}
\begin{tabular}{l r  l r}
\specialrule{.1em}{0em}{.2em}
\specialrule{.1em}{0em}{1em}
    \Large \textsc{Typing} & 
    &  & \framebox{$\Gamma \vdash e : \tau$}\\
    & & \\
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$x:\tau \in \Gamma$}
            \RightLabel{\textsc{(T-Var)}}
        \UnaryInfC{$\Gamma \vdash x : \tau$}
        \DisplayProof
    } &
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$\Gamma,\alpha \vdash e : \tau$}
            \RightLabel{\textsc{(T-TAbs)}}
        \UnaryInfC{$\Gamma \vdash \Lambda \alpha.e:\forall\alpha.\tau$}
        \DisplayProof
    }
    \\
    & &\\
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$\Gamma,x{:}\tau_1 \vdash e_2 : \tau_2$}
            \RightLabel{\textsc{(T-Abs)}}
        \UnaryInfC{$\Gamma \vdash \lam x{:}\tau_1.e_2:\tau_1 \to \tau_2$}
        \DisplayProof
    } &
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$\Gamma \vdash e : \forall\alpha.\tau_1$}
            \RightLabel{\textsc{(T-TApp)}}
        \UnaryInfC{$\Gamma \vdash e\lceil\tau_2\rceil : [\tau_2/\alpha]\tau_1$}
        \DisplayProof
    }
    \\
    & &\\
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$\Gamma \vdash e_1 : \tau_1 \to \tau_2$}
        \AxiomC{$\Gamma \vdash e_2 : \tau_1$}
            \RightLabel{\textsc{(T-App)}}
        \BinaryInfC{$\Gamma \vdash e_1e_2 : \tau_2$}
        \DisplayProof
    } \\
    & \\
\specialrule{.1em}{1em}{0em}
\end{tabular}
\caption{Typing in System F}
    \label{fig:typing}
\end{figure}

\begin{theorem}[\textsc{Progress in Typing}]
If $e$ is a closed, well-typed program, then either $e$ is a value or else there is some program $e'$ such that $e \to_\beta e'$.
\label{progress-typing}
\end{theorem}
\begin{theorem}[\textsc{Preservation in Typing}]
If $\,\Gamma \vdash e : \tau$ and $e \to_\beta e'$, then $\Gamma \vdash e' : \tau$.
\label{preservation-typing}
\end{theorem} 

\begin{figure}[h]
\centering
\setlength{\tabcolsep}{12pt}
\begin{tabular}{l r  l r}
\specialrule{.1em}{0em}{.2em}
\specialrule{.1em}{0em}{1em}
    \Large \textsc{Evaluating} & 
    &  & \fbox{ $e \to_\beta e'$}\\
    & & \\
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$e_1 \to_\beta e_1'$}
            \RightLabel{\textsc{(E-App1)}}
        \UnaryInfC{$e_1e_2 \to_\beta e_1'e_2$}
        \DisplayProof
    } &
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$e \to_\beta e'$}
            \RightLabel{\textsc{(E-TApp)}}
        \UnaryInfC{$e\lceil\tau\rceil \to_\beta e'\lceil\tau\rceil$}
        \DisplayProof
    }
    \\
    & &\\
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$e_2 \to_\beta e_2'$}
            \RightLabel{\textsc{(E-App2)}}
        \UnaryInfC{$e_1e_2 \to_\beta e_1e_2'$}
        \DisplayProof
    } &
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$(\Lambda\alpha.\lam x{:}\alpha.e)\lceil\tau\rceil \to_\beta (\lam x{:}\alpha.e)[\tau/\alpha]\,\,$\textsc{(E-TSub)}}
        \DisplayProof
    }
    \\
    & &\\
    \multicolumn{2}{c}{
    \def\extraVskip{4pt}
    \def\labelSpacing{4pt}
    \def\defaultHypSeparation{\hskip .05in}
        \AxiomC{$(\lam x{:}\tau.e)v \to_\beta e[v/x]\,\,$\textsc{(E-Sub)}}
        \DisplayProof
    } \\
    & \\
\specialrule{.1em}{1em}{0em}
\end{tabular}
\caption{Evaluating in System F}
    \label{fig:evaluating}
\end{figure}

\begin{theorem}[\textsc{Normalization in Evaluation}]
Well-typed programs in System F always evaluate to a value, to a normal form.
\label{normalization-evaluation}
\end{theorem}
%%Text of appendix \ldots

\end{document}
